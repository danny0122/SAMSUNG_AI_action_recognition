{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import PIL.Image as Image\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from os.path import join\n",
    "import time\n",
    "import pickle\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#Run the code using selected GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1, 2, 3\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TRAIN_CHECK_POINT = 'check_point/'\n",
    "\n",
    "#Experiment, Optimization options\n",
    "DATA_SPLIT_PATH = 'data_split.pkl'\n",
    "BATCH_SIZE = 10\n",
    "NUM_CLASSES = 11\n",
    "CROP_SIZE = 112\n",
    "CHANNEL_NUM = 3\n",
    "CLIP_LENGTH = 16\n",
    "EPOCH_NUM = 50\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing : Define UCF11Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_LENGTH = 16\n",
    "\n",
    "np_mean = np.load('crop_mean.npy').reshape([CLIP_LENGTH, 112, 112, 3])\n",
    "\n",
    "def get_test_num(filename):\n",
    "    lines = open(filename, 'r')\n",
    "    return len(list(lines))\n",
    "\n",
    "def frame_process(clip, clip_length=CLIP_LENGTH, crop_size=112, channel_num=3):\n",
    "    frames_num = len(clip)\n",
    "    croped_frames = np.zeros([frames_num, crop_size, crop_size, channel_num]).astype(np.float32)\n",
    "\n",
    "\n",
    "    #Crop every frame into shape[crop_size, crop_size, channel_num]\n",
    "    for i in range(frames_num):\n",
    "        img = Image.fromarray(clip[i].astype(np.uint8))\n",
    "        if img.width > img.height:\n",
    "            scale = float(crop_size) / float(img.height)\n",
    "            img = np.array(cv2.resize(np.array(img), (int(img.width * scale + 1), crop_size))).astype(np.float32)\n",
    "        else:\n",
    "            scale = float(crop_size) / float(img.width)\n",
    "            img = np.array(cv2.resize(np.array(img), (crop_size, int(img.height * scale + 1)))).astype(np.float32)\n",
    "        crop_x = int((img.shape[0] - crop_size) / 2)\n",
    "        crop_y = int((img.shape[1] - crop_size) / 2)\n",
    "        img = img[crop_x: crop_x + crop_size, crop_y : crop_y + crop_size, :]\n",
    "        croped_frames[i, :, :, :] = img - np_mean[i]\n",
    "\n",
    "    return croped_frames\n",
    "\n",
    "\n",
    "def convert_images_to_clip(filename, clip_length=CLIP_LENGTH, crop_size=112, channel_num=3):\n",
    "    clip = []\n",
    "    for parent, dirnames, filenames in os.walk(filename):\n",
    "        filenames = sorted(filenames)\n",
    "        if len(filenames) < clip_length:\n",
    "            for i in range(0, len(filenames)):\n",
    "                image_name = str(filename) + '/' + str(filenames[i])\n",
    "                img = Image.open(image_name)\n",
    "                img_data = np.array(img)\n",
    "                clip.append(img_data)\n",
    "            for i in range(clip_length - len(filenames)):\n",
    "                image_name = str(filename) + '/' + str(filenames[len(filenames) - 1])\n",
    "                img = Image.open(image_name)\n",
    "                img_data = np.array(img)\n",
    "                clip.append(img_data)\n",
    "        else:\n",
    "            s_index = random.randint(0, len(filenames) - clip_length)\n",
    "            for i in range(s_index, s_index + clip_length):\n",
    "                image_name = str(filename) + '/' + str(filenames[i])\n",
    "                img = Image.open(image_name)\n",
    "                img_data = np.array(img)\n",
    "                clip.append(img_data)\n",
    "    if len(clip) == 0:\n",
    "        print(filename)\n",
    "    clip = frame_process(clip, clip_length, crop_size, channel_num)\n",
    "    return clip # shape: [clip_length, crop_size, crop_size, channel_num]\n",
    "\n",
    "class UCF11Dataset(Dataset):\n",
    "    def __init__(self, data_list, num_classes, crop_size=112, channel_num=3):\n",
    "        self.data_list = data_list\n",
    "        self.video_list = list(data_list)\n",
    "        self.crop_size = crop_size\n",
    "        self.channel_num = channel_num        \n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_list)\n",
    "    \n",
    "    def __getitem__(self, i):                \n",
    "        line = self.video_list[i].strip('\\n').split()\n",
    "        dirname = line[0]\n",
    "        label = int(self.data_list[dirname])\n",
    "        clips = convert_images_to_clip(dirname, CLIP_LENGTH, self.crop_size, self.channel_num)              \n",
    "        \n",
    "        clips = np.transpose(np.array(clips).astype(np.float32), (3, 0, 1, 2))\n",
    "        \n",
    "        batch_data = {'clips': clips, 'labels': label}\n",
    "        \n",
    "        return batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load UCF11(UCF YouTube Action) Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT_PATH = 'data_split.pkl'\n",
    "ucf11_dataset = pickle.load(open(DATA_SPLIT_PATH,'rb'))\n",
    "train_set = ucf11_dataset['train']\n",
    "test_set = ucf11_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video_dataset = UCF11Dataset(train_set, NUM_CLASSES)\n",
    "test_video_dataset = UCF11Dataset(test_set, NUM_CLASSES)\n",
    "\n",
    "train_video_dataloader = DataLoader(train_video_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_video_dataloader = DataLoader(test_video_dataset, batch_size = BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define NonLocal Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLocalBlock3D(nn.Module):\n",
    "    def __init__(self, in_channels, test_mode=False, dimension=3, sub_sample=True):\n",
    "        super(NonLocalBlock3D, self).__init__()\n",
    "        \n",
    "        self.test_mode = test_mode\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample = sub_sample\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.inter_channels = in_channels // 2\n",
    "        if self.inter_channels == 0:\n",
    "            self.inter_channels = 1\n",
    "\n",
    "        max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "       \n",
    "        #============================================================\n",
    "        #make self.g , self.theta, self.phi\n",
    "        #these are nn.Conv3d, 1x1x1, stride=1, padding=0\n",
    "        #============================================================\n",
    "        self.g = nn.Conv3d(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                         kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.theta = nn.Conv3d(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.phi = nn.Conv3d(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "        #============================================================\n",
    "\n",
    "        #============================================================\n",
    "        #make self.W\n",
    "        #in this part, self.W.weight and self.W.bias must initialize to 0\n",
    "        #============================================================\n",
    "        self.W = nn.Conv3d(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                         kernel_size=1, stride=1, padding=0)\n",
    "        nn.init.constant_(self.W.weight, 0)\n",
    "        nn.init.constant_(self.W.bias, 0)\n",
    "        #============================================================\n",
    "\n",
    "        if sub_sample:\n",
    "            self.g = nn.Sequential(self.g, max_pool_layer)\n",
    "            self.phi = nn.Sequential(self.phi, max_pool_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (b, c, t, h, w)\n",
    "        :return:\n",
    "        '''\n",
    "        batch_size = x.size(0)\n",
    "        #============================================================\n",
    "        #1. use self.g(x)\n",
    "        #2. use self.theta(x)\n",
    "        #3. use self.phi(x)\n",
    "        #4. several matrix multiplication between previous return value\n",
    "        #5. use self.W(y)\n",
    "        #6. make z with x and self.W(y)\n",
    "        #============================================================\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1)\n",
    "        \n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "\n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        f_div_C = F.softmax(f, dim=-1)\n",
    "\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        \n",
    "        if self.test_mode:\n",
    "            print(\"x: {}\".format(x.shape))\n",
    "            print(\"g_x: {}\".format(g_x.shape))\n",
    "            print(\"theta_x: {}\".format(theta_x.shape))\n",
    "            print(\"phi_x: {}\".format(phi_x.shape))\n",
    "            print(\"f: {}\".format(f.shape))\n",
    "            print(\"y: {}\".format(y.shape))\n",
    "\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "        #============================================================\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define C3D Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C3D(nn.Module):\n",
    "    \"\"\"\n",
    "    The C3D network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, pretrained=\"\"):\n",
    "        super(C3D, self).__init__()\n",
    "        \n",
    "        #============================================================\n",
    "        #All of convolution layers use kernel_size (3,3,3) and padding (1, 1, 1)\n",
    "        #conv1 3 -> 64\n",
    "        #conv2 64 -> 128\n",
    "        #conv3a 128 -> 256\n",
    "        #conv3b 256 -> 256\n",
    "        #conv4a 256 -> 512\n",
    "        #conv4b 512 -> 512\n",
    "        #conv5a 512 -> 512\n",
    "        #conv5b 512 -> 512\n",
    "        #fc6 (you need to find input channel size) -> 4096\n",
    "        #fc7 4096 -> num_classes\n",
    "        #============================================================\n",
    "\n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.nonlocal1 = NonLocalBlock3D(64)\n",
    "\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        self.nonlocal2 = NonLocalBlock3D(128)\n",
    "\n",
    "        self.conv3a = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv3b = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        self.nonlocal3 = NonLocalBlock3D(256)\n",
    "\n",
    "        self.conv4a = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv4b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        self.nonlocal4 = NonLocalBlock3D(512)\n",
    "\n",
    "        self.conv5a = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv5b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))\n",
    "\n",
    "        self.fc6 = nn.Linear(8192, 4096)\n",
    "        self.fc7 = nn.Linear(4096, num_classes)\n",
    "        #============================================================\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.__init_weight()\n",
    "\n",
    "        if pretrained:\n",
    "            self.__load_pretrained_weights(pretrained)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #============================================================\n",
    "        #use all layer to forward\n",
    "        #============================================================\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        #x = self.nonlocal1(x)\n",
    "\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        #x = self.nonlocal2(x)\n",
    "\n",
    "        x = self.relu(self.conv3a(x))\n",
    "        x = self.relu(self.conv3b(x))\n",
    "        x = self.pool3(x)\n",
    "        #x = self.nonlocal3(x)\n",
    "\n",
    "        x = self.relu(self.conv4a(x))\n",
    "        x = self.relu(self.conv4b(x))\n",
    "        x = self.pool4(x)\n",
    "        #x = self.nonlocal4(x)\n",
    "\n",
    "        x = self.relu(self.conv5a(x))\n",
    "        x = self.relu(self.conv5b(x))\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        x = x.view(-1, 8192)\n",
    "        x = self.relu(self.fc6(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #============================================================\n",
    "        logits = self.fc7(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def __load_pretrained_weights(self, model_path):\n",
    "        \"\"\"Initialiaze network.\"\"\"\n",
    "        corresp_name = {\n",
    "                        # Conv1\n",
    "                        \"features.0.weight\": \"conv1.weight\",\n",
    "                        \"features.0.bias\": \"conv1.bias\",\n",
    "                        # Conv2\n",
    "                        \"features.3.weight\": \"conv2.weight\",\n",
    "                        \"features.3.bias\": \"conv2.bias\",\n",
    "                        # Conv3a\n",
    "                        \"features.6.weight\": \"conv3a.weight\",\n",
    "                        \"features.6.bias\": \"conv3a.bias\",\n",
    "                        # Conv3b\n",
    "                        \"features.8.weight\": \"conv3b.weight\",\n",
    "                        \"features.8.bias\": \"conv3b.bias\",\n",
    "                        # Conv4a\n",
    "                        \"features.11.weight\": \"conv4a.weight\",\n",
    "                        \"features.11.bias\": \"conv4a.bias\",\n",
    "                        # Conv4b\n",
    "                        \"features.13.weight\": \"conv4b.weight\",\n",
    "                        \"features.13.bias\": \"conv4b.bias\",\n",
    "                        # Conv5a\n",
    "                        \"features.16.weight\": \"conv5a.weight\",\n",
    "                        \"features.16.bias\": \"conv5a.bias\",\n",
    "                         # Conv5b\n",
    "                        \"features.18.weight\": \"conv5b.weight\",\n",
    "                        \"features.18.bias\": \"conv5b.bias\",\n",
    "                        # fc6\n",
    "                        \"classifier.0.weight\": \"fc6.weight\",\n",
    "                        \"classifier.0.bias\": \"fc6.bias\",\n",
    "                        # fc7\n",
    "                        \"classifier.3.weight\": \"fc7.weight\",\n",
    "                        \"classifier.3.bias\": \"fc7.bias\",\n",
    "                        }\n",
    "\n",
    "        p_dict = torch.load(model_path)['state_dict']\n",
    "        s_dict = self.state_dict()\n",
    "        for name in p_dict:\n",
    "            if name not in corresp_name:\n",
    "                continue\n",
    "            s_dict[corresp_name[name]] = p_dict[name]\n",
    "        self.load_state_dict(s_dict)\n",
    "\n",
    "    def __init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Network and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = C3D(num_classes=NUM_CLASSES)\n",
    "net = net.cuda()\n",
    "\n",
    "#net = C3D(num_classes=NUM_CLASSES).cuda()\n",
    "#net = torch.nn.DataParallel(net).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test C3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0: Loss is 68.08154; Accuracy is 0.00000\n",
      "Epoch 1, Batch 10: Loss is 4.60630; Accuracy is 0.10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m loss_epoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m net\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_video_dataloader):        \n\u001b[1;32m      8\u001b[0m     batch_clips \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mclips\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m     batch_labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/samsung/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/samsung/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/samsung/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/samsung/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m, in \u001b[0;36mUCF11Dataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     71\u001b[0m dirname \u001b[39m=\u001b[39m line[\u001b[39m0\u001b[39m]\n\u001b[1;32m     72\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_list[dirname])\n\u001b[0;32m---> 73\u001b[0m clips \u001b[39m=\u001b[39m convert_images_to_clip(dirname, CLIP_LENGTH, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrop_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchannel_num)              \n\u001b[1;32m     75\u001b[0m clips \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(np\u001b[39m.\u001b[39marray(clips)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32), (\u001b[39m3\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m     77\u001b[0m batch_data \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mclips\u001b[39m\u001b[39m'\u001b[39m: clips, \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m: label}\n",
      "Cell \u001b[0;32mIn[2], line 55\u001b[0m, in \u001b[0;36mconvert_images_to_clip\u001b[0;34m(filename, clip_length, crop_size, channel_num)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(clip) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     54\u001b[0m     \u001b[39mprint\u001b[39m(filename)\n\u001b[0;32m---> 55\u001b[0m clip \u001b[39m=\u001b[39m frame_process(clip, clip_length, crop_size, channel_num)\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m clip\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mframe_process\u001b[0;34m(clip, clip_length, crop_size, channel_num)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m#Crop every frame into shape[crop_size, crop_size, channel_num]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(frames_num):\n\u001b[0;32m---> 16\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(clip[i]\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49muint8))\n\u001b[1;32m     17\u001b[0m     \u001b[39mif\u001b[39;00m img\u001b[39m.\u001b[39mwidth \u001b[39m>\u001b[39m img\u001b[39m.\u001b[39mheight:\n\u001b[1;32m     18\u001b[0m         scale \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(crop_size) \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(img\u001b[39m.\u001b[39mheight)\n",
      "File \u001b[0;32m~/anaconda3/envs/samsung/lib/python3.8/site-packages/PIL/Image.py:3013\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3010\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3011\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mtostring()\n\u001b[0;32m-> 3013\u001b[0m \u001b[39mreturn\u001b[39;00m frombuffer(mode, size, obj, \u001b[39m\"\u001b[39;49m\u001b[39mraw\u001b[39;49m\u001b[39m\"\u001b[39;49m, rawmode, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/samsung/lib/python3.8/site-packages/PIL/Image.py:2940\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2937\u001b[0m         im\u001b[39m.\u001b[39mreadonly \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2938\u001b[0m         \u001b[39mreturn\u001b[39;00m im\n\u001b[0;32m-> 2940\u001b[0m \u001b[39mreturn\u001b[39;00m frombytes(mode, size, data, decoder_name, args)\n",
      "File \u001b[0;32m~/anaconda3/envs/samsung/lib/python3.8/site-packages/PIL/Image.py:2881\u001b[0m, in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2878\u001b[0m \u001b[39mif\u001b[39;00m decoder_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m args \u001b[39m==\u001b[39m ():\n\u001b[1;32m   2879\u001b[0m     args \u001b[39m=\u001b[39m mode\n\u001b[0;32m-> 2881\u001b[0m im \u001b[39m=\u001b[39m new(mode, size)\n\u001b[1;32m   2882\u001b[0m im\u001b[39m.\u001b[39mfrombytes(data, decoder_name, args)\n\u001b[1;32m   2883\u001b[0m \u001b[39mreturn\u001b[39;00m im\n",
      "File \u001b[0;32m~/anaconda3/envs/samsung/lib/python3.8/site-packages/PIL/Image.py:2845\u001b[0m, in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2843\u001b[0m     im\u001b[39m.\u001b[39mpalette \u001b[39m=\u001b[39m ImagePalette\u001b[39m.\u001b[39mImagePalette()\n\u001b[1;32m   2844\u001b[0m     color \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpalette\u001b[39m.\u001b[39mgetcolor(color)\n\u001b[0;32m-> 2845\u001b[0m \u001b[39mreturn\u001b[39;00m im\u001b[39m.\u001b[39m_new(core\u001b[39m.\u001b[39;49mfill(mode, size, color))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH_NUM):\n",
    "    # train\n",
    "    correct_epoch = 0\n",
    "    loss_epoch = 0\n",
    "    net.train()\n",
    "    \n",
    "    for i, batch in enumerate(train_video_dataloader):        \n",
    "        batch_clips = batch['clips']\n",
    "        batch_labels = batch['labels']\n",
    "        batch_clips = batch_clips.cuda()\n",
    "        batch_labels = batch_labels.cuda()\n",
    "        \n",
    "        logits = net(batch_clips)                \n",
    "\n",
    "        loss = F.cross_entropy(logits, batch_labels)\n",
    "        correct = (torch.argmax(logits, 1) == batch_labels).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_epoch += loss\n",
    "        correct_epoch += correct\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Epoch %d, Batch %d: Loss is %.5f; Accuracy is %.5f'%(epoch+1, i, loss, correct/batch_clips.shape[0]))\n",
    "            \n",
    "    print('Epoch %d: Average loss is: %.5f; Average accuracy is: %.5f'%(epoch+1, loss_epoch / len(train_video_dataloader),\n",
    "                                                                                correct_epoch / len(train_video_dataset)))\n",
    "                \n",
    "    # test\n",
    "    correct_epoch = 0\n",
    "    loss_epoch = 0\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_video_dataloader):\n",
    "            batch_clips = batch['clips']\n",
    "            batch_labels = batch['labels']\n",
    "            batch_clips = batch_clips.cuda()\n",
    "            batch_labels = batch_labels.cuda()\n",
    "\n",
    "            logits = net(batch_clips)\n",
    "\n",
    "            loss = F.cross_entropy(logits, batch_labels)\n",
    "            correct = (torch.argmax(logits, 1) == batch_labels).sum()    \n",
    "\n",
    "            loss_epoch += loss\n",
    "            correct_epoch += correct\n",
    "        \n",
    "    print('Test loss is %.5f; Accuracy is %.5f'%(loss_epoch / len(test_video_dataloader),\n",
    "                                                                                correct_epoch / len(test_video_dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('samsung': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e7c3f763e218c31241ebf3a820e60d8ad9990019c62e2b485b53b4ae85ffac8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
